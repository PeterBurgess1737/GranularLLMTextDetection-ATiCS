{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ROC With Majority Vote",
   "id": "e14390e0e741c32a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import pathlib\n",
    "import statistics\n",
    "from typing import Any, Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scripts.load_working_set import load_working_set\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "e72984cf75f8cabd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some constants.",
   "id": "391a50ce56a2069"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The location of the data\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: '{DATA_DIR}'\")"
   ],
   "id": "c5b375cf78ed15df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Grabbing our data.",
   "id": "291b42cfb6435aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load our data\n",
    "sentence_evaluations, sentence_realities = load_working_set(\"fine-tuning\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "d2640f7d0dbaffc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a working set of thresholds to calculate the ROC/AUROC for.",
   "id": "69cc4e7f52a62b2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Flatten our sentence evaluations array\n",
    "flat_sentence_evaluations = list(itertools.chain.from_iterable(sentence_evaluations))\n",
    "\n",
    "# Extract only the unique percentages\n",
    "flat_sentence_evaluations = list(set(flat_sentence_evaluations))\n",
    "\n",
    "# Sort this\n",
    "flat_sentence_evaluations.sort()\n",
    "\n",
    "# Calculate the distinct thresholds\n",
    "distinct_thresholds = []\n",
    "for i in range(len(flat_sentence_evaluations) - 1):\n",
    "    midpoint = (flat_sentence_evaluations[i] + flat_sentence_evaluations[i + 1]) / 2\n",
    "    distinct_thresholds.append(midpoint)\n",
    "\n",
    "# Also add the minimum and maximum\n",
    "if 0 not in distinct_thresholds:\n",
    "    distinct_thresholds.append(0)\n",
    "if 1 not in distinct_thresholds:\n",
    "    distinct_thresholds.append(1)"
   ],
   "id": "64ce7dafc9626c69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A little prediction function to implement the majority approach.",
   "id": "3e4768fd49883781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(_sentence_evaluations: list[float], _threshold: float) -> float:\n",
    "    _votes = [\n",
    "        True if _evaluation >= _threshold else False\n",
    "        for _evaluation in _sentence_evaluations\n",
    "    ]\n",
    "\n",
    "    # Average them\n",
    "    _prediction = statistics.mean(_votes)\n",
    "\n",
    "    return _prediction\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "cb6d6e1d8011f2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we create the evaluations from each percentage, generate a ROC curve and AUROC, then store it.",
   "id": "403a122fd31e6cea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_data_keys = Literal[\"AUROC\", \"fpr\", \"tpr\", \"thresholds\"]\n",
    "threshold_to_data: dict[float, dict[_data_keys, Any]] = {}\n",
    "\n",
    "# For each threshold\n",
    "for current_threshold in tqdm(distinct_thresholds, desc=\"Calculating AUROC for each threshold\"):\n",
    "    # Generate the predictions for each sentence\n",
    "    sentence_predictions = []\n",
    "    for evaluations in sentence_evaluations:\n",
    "        sentence_predictions.append(predict(evaluations, current_threshold))\n",
    "\n",
    "    # Generate the ROC and AUROC for this threshold\n",
    "    fpr, tpr, all_vote_thresholds = roc_curve(sentence_realities, sentence_predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Store\n",
    "    threshold_to_data[current_threshold] = {\n",
    "        \"AUROC\": roc_auc,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"thresholds\": all_vote_thresholds\n",
    "    }\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "551c902ceb7166a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finding the best threshold value via AUROC and graphing it.",
   "id": "44830b4576331190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_vote_threshold, best_vote_threshold_data = max(threshold_to_data.items(), key=lambda x: x[1][\"AUROC\"])\n",
    "best_vote_threshold_auroc = best_vote_threshold_data[\"AUROC\"]\n",
    "\n",
    "# Extract threshold and AUROC values for plotting\n",
    "all_vote_thresholds = list(threshold_to_data.keys())\n",
    "all_vote_data = [_data[\"AUROC\"] for _data in threshold_to_data.values()]\n",
    "\n",
    "# Sort by thresholds for a smoother plot\n",
    "sorted_vote_thresholds, sorted_vote_auroc_values = zip(*sorted(zip(all_vote_thresholds, all_vote_data)))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_vote_thresholds, sorted_vote_auroc_values, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"AUROC\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Highlight the best threshold point with a vertical line and red marker\n",
    "plt.axvline(x=best_vote_threshold, color='red', linestyle='--', label=f\"Best Threshold: {best_vote_threshold:.2f}\")\n",
    "plt.scatter(best_vote_threshold, best_vote_threshold_auroc, color='red', s=100)  # Red point at best threshold\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Set the title\n",
    "title = \"AUROC vs Vote Threshold for Majority Vote Approach\"\n",
    "plt.title(title)\n",
    "\n",
    "# Also save it as a file\n",
    "file = DATA_DIR / (title.replace(\" \", \"_\") + \".png\")\n",
    "plt.savefig(file)\n",
    "\n",
    "# And show the plot\n",
    "plt.show()"
   ],
   "id": "eddd911fbd69757a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Graphic the ROC curve for this best threshold.  \n",
    "Then use Youden's J Statistic to find the best threshold to apply to the final value. "
   ],
   "id": "5654d72d0e8e5886"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the best threshold for this threshold, kinda confusing I know...\n",
    "youden_index = best_vote_threshold_data[\"tpr\"] - best_vote_threshold_data[\"fpr\"]\n",
    "best_threshold_youden_index = np.argmax(youden_index)\n",
    "best_threshold_youden = best_vote_threshold_data[\"thresholds\"][best_threshold_youden_index]\n",
    "\n",
    "# And plot\n",
    "plt.figure()\n",
    "plt.plot(best_vote_threshold_data[\"fpr\"], best_vote_threshold_data[\"tpr\"],\n",
    "         color=\"darkorange\", lw=2,\n",
    "         label=f\"ROC curve (AUROC = {best_vote_threshold_data[\"AUROC\"]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "\n",
    "# Plot the point of the best threshold\n",
    "plt.scatter(best_vote_threshold_data[\"fpr\"][best_threshold_youden_index],\n",
    "            best_vote_threshold_data[\"tpr\"][best_threshold_youden_index],\n",
    "            color=\"red\", label=f\"Best Threshold (J={best_threshold_youden:.2f})\")\n",
    "# And draw a vertical line underneath\n",
    "plt.plot(\n",
    "    [\n",
    "        best_vote_threshold_data[\"fpr\"][best_threshold_youden_index],\n",
    "        best_vote_threshold_data[\"fpr\"][best_threshold_youden_index]\n",
    "    ],\n",
    "    [\n",
    "        best_vote_threshold_data[\"tpr\"][best_threshold_youden_index],\n",
    "        best_vote_threshold_data[\"fpr\"][best_threshold_youden_index]\n",
    "    ],\n",
    "    \"r--\"\n",
    ")\n",
    "\n",
    "# Legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "# Title\n",
    "title = f\"Receiver Operating Characteristic (ROC) for Vote Threshold {best_vote_threshold:.2f}\"\n",
    "plt.title(title)\n",
    "\n",
    "# Save the file\n",
    "file = DATA_DIR / (title.replace(\" \", \"_\") + \".png\")\n",
    "plt.savefig(file)\n",
    "\n",
    "# Display\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best threshold = {best_threshold_youden:.2f}\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "70668a53b6b3adf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now some further evaluation.",
   "id": "851e31ef34fa3c18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentence_evaluations_eval, sentence_realities_eval = load_working_set(\"evaluation\")\n",
    "\n",
    "sentence_predictions_eval: list[float] = []\n",
    "for evaluations in sentence_evaluations_eval:\n",
    "    sentence_predictions_eval.append(predict(evaluations, best_vote_threshold))\n",
    "\n",
    "# Apply the threshold\n",
    "classified_sentences_eval = [\n",
    "    1 if p >= best_vote_threshold else 0 for p in sentence_predictions_eval\n",
    "]\n",
    "\n",
    "# And calculate some performance metrics\n",
    "accuracy = accuracy_score(sentence_realities_eval, classified_sentences_eval)\n",
    "precision = precision_score(sentence_realities_eval, classified_sentences_eval)\n",
    "recall = recall_score(sentence_realities_eval, classified_sentences_eval)\n",
    "f1 = f1_score(sentence_realities_eval, classified_sentences_eval)\n",
    "\n",
    "# Tells us the secrets\n",
    "print(f\"For vote threshold {best_vote_threshold:.2f}\")\n",
    "print(f\"For final threshold {best_threshold_youden:.2f}\\n\")\n",
    "print(f\"\\tAccuracy:  {accuracy:.2f}\")\n",
    "print(f\"\\tPrecision: {precision:.2f}\")\n",
    "print(f\"\\tRecall:    {recall:.2f}\")\n",
    "print(f\"\\tF1 Score:  {f1:.2f}\")\n",
    "\n",
    "# File to store this\n",
    "results_file = DATA_DIR / \"roc_vote_results.txt\"\n",
    "with results_file.open(\"w\") as f:\n",
    "    print(f\"For vote threshold {best_vote_threshold:.2f}\", file=f)\n",
    "    print(f\"For final threshold {best_threshold_youden:.2f}\\n\", file=f)\n",
    "    print(f\"\\tAccuracy:  {accuracy:.2f}\", file=f)\n",
    "    print(f\"\\tPrecision: {precision:.2f}\", file=f)\n",
    "    print(f\"\\tRecall:    {recall:.2f}\", file=f)\n",
    "    print(f\"\\tF1 Score:  {f1:.2f}\", file=f)\n",
    "\n",
    "print()\n",
    "print(\"Done\")"
   ],
   "id": "acfef6a91488d3f7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
