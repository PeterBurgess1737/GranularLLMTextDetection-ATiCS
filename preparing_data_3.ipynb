{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing Data\n",
    "\n",
    "Here we grab the data used from the Human vs LLM Text Corpus, extract a subset we want to use, modify some of it and then save it to a series of files.  \n",
    "Not necessarily in that order, but you get the idea.\n"
   ],
   "id": "8d9ddb36e5263889"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the csv file.",
   "id": "f071c62bf1c074a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_df = pd.read_csv(pathlib.Path(\"data/original/Human vs LLM Text Corpus/data.csv\"))\n",
    "prompts_df = pd.read_csv(pathlib.Path(\"data/original/Human vs LLM Text Corpus/prompts.csv\"))\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "dbdd49e5deea0245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Extracting the data we want.\n",
    "\n",
    "* Select only Human texts.\n",
    "* Add the prompt to each row of human text.\n",
    "* Remove all columns but the ones we care about.\n",
    "* Cleaning the text a little."
   ],
   "id": "fb2c0cfa5cc1df8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Grab the human sources\n",
    "human_sources = data_df[data_df[\"source\"] == \"Human\"]\n",
    "\n",
    "# Add in the prompts\n",
    "\n",
    "# Rename the column in the prompts dataframe to make things easier\n",
    "prompts_df = prompts_df.rename(columns={\"Prompt ID\": \"prompt_id\"})\n",
    "# Merge the dataframes\n",
    "human_sources = human_sources.merge(prompts_df[['prompt_id', 'Prompt']], on='prompt_id', how='left')\n",
    "# Rename the Prompt column to prompt as I like lowercase more\n",
    "human_sources = human_sources.rename(columns={\"Prompt\": \"prompt\"})\n",
    "# Remove any whose prompt is undefined\n",
    "human_sources = human_sources[human_sources[\"prompt\"] != \"Undefined\"]\n",
    "\n",
    "# Grab only the text and prompt columns\n",
    "human_sources = human_sources.loc[:, [\"text\", \"prompt\"]]\n",
    "\n",
    "# Strip the leading and trailing whitespace from the texts\n",
    "human_sources[\"text\"] = human_sources[\"text\"].str.strip()\n",
    "human_sources[\"text\"] = human_sources[\"text\"].str.replace(\"Â \", \" \")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "ef4ccccaaf718ee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adding a new column for each sentence and another for the number of sentences for each text.  \n",
    "This takes a bit because of the number of human texts."
   ],
   "id": "866c87c8f878d187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = nltk.tokenize.PunktTokenizer()\n",
    "\n",
    "\n",
    "def _modify(_row: pd.Series):\n",
    "    _sentence_spans = list(tokenizer.span_tokenize(_row[\"text\"]))\n",
    "    _sentences = [\n",
    "        _row[\"text\"][start:stop] for start, stop in _sentence_spans\n",
    "    ]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"sentence_spans\": _sentence_spans,\n",
    "        \"sentences\": _sentences\n",
    "    })\n",
    "\n",
    "\n",
    "# Create a row for the spans of the sentences and the actual sentences\n",
    "tqdm.pandas(desc=\"Tokenizing texts to sentences\")\n",
    "human_sources[[\"sentence_spans\", \"sentences\"]] = human_sources.progress_apply(\n",
    "    _modify,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# And another row for the number of sentences\n",
    "tqdm.pandas(desc=\"Counting the number of sentences in each text\")\n",
    "human_sources[\"num_sentences\"] = human_sources[\"sentences\"].apply(\n",
    "    lambda _sentences: len(_sentences)\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "5c8c7c59494ff52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Filter to the texts that we want.  \n",
    "That is texts with more than 40 sentences,\n",
    "50 of each with the number of sentences from 40 to 60,\n",
    "inclusive of 40, but not 60.  \n",
    "Why this?\n",
    "Well, because I wanted some even longish data."
   ],
   "id": "ff2e084358cff230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MIN_NUM_SENTENCES = 40\n",
    "MAX_NUM_SENTENCES = 60\n",
    "NUM_PER_SUBSET = 50\n",
    "\n",
    "picked_sentences_list = []\n",
    "for num in range(MIN_NUM_SENTENCES, MAX_NUM_SENTENCES):\n",
    "    _subset = human_sources[human_sources[\"num_sentences\"] == num]\n",
    "    if len(_subset) < NUM_PER_SUBSET:\n",
    "        picked_sentences_list.append(_subset)\n",
    "    else:\n",
    "        picked_sentences_list.append(_subset.sample(n=50, random_state=42))\n",
    "picked_sentences = pd.concat(picked_sentences_list)\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "bb4db9cacbeae4c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we select 50% of the data to be modified in some way by an AI.",
   "id": "ecb5d38d9a7d14b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unmodified_texts = []\n",
    "to_be_ai_modified_texts = []\n",
    "\n",
    "for num in range(MIN_NUM_SENTENCES, MAX_NUM_SENTENCES):\n",
    "    subset = picked_sentences[picked_sentences[\"num_sentences\"] == num]\n",
    "\n",
    "    unmodified = subset.sample(frac=0.5, random_state=42)\n",
    "    to_be_ai_modified = subset.drop(unmodified.index)\n",
    "\n",
    "    unmodified_texts.append(unmodified)\n",
    "    to_be_ai_modified_texts.append(to_be_ai_modified)\n",
    "\n",
    "unmodified_texts = pd.concat(unmodified_texts)\n",
    "to_be_ai_modified_texts = pd.concat(to_be_ai_modified_texts)\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "1d2a212e6a2c478f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Due to working with a LLM, some of the response may not be in the correct format as requested.  \n",
    "This is a simple dat to track the types of failures that occur during modification."
   ],
   "id": "7af2532e44844e56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_skips = 0\n",
    "reasons_for_skipping_rows: dict[str, int] = {}\n",
    "last_skip_reason: str = \"\"\n",
    "\n",
    "\n",
    "def log_row_skip(_reason_for_skip: str):\n",
    "    global num_skips, last_skip_reason\n",
    "\n",
    "    if _reason_for_skip not in reasons_for_skipping_rows:\n",
    "        reasons_for_skipping_rows[_reason_for_skip] = 0\n",
    "    reasons_for_skipping_rows[_reason_for_skip] += 1\n",
    "\n",
    "    num_skips += 1\n",
    "    last_skip_reason = _reason_for_skip\n",
    "\n",
    "\n",
    "def print_failures():\n",
    "    print(\"Types of failures:\")\n",
    "    padding = max([\n",
    "        len(_key) for _key in reasons_for_skipping_rows.keys()\n",
    "    ])\n",
    "\n",
    "    for _key, _value in reasons_for_skipping_rows.items():\n",
    "        print(f\"\\t{_key:<{padding}} : {_value}\")\n",
    "\n",
    "\n",
    "def reset_skip_logging():\n",
    "    global num_skips, reasons_for_skipping_rows, last_skip_reason\n",
    "\n",
    "    num_skips = 0\n",
    "    reasons_for_skipping_rows = {}\n",
    "    last_skip_reason = \"\"\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "9f0e99b0798f599e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Helper function to select sentences to be modified.\n",
    "\n",
    "Either you can select sentences randomly, or in a cluster.\n",
    "This function simply selects `n` items from the given list."
   ],
   "id": "e52056b797b48e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def select_sentences(_sentences: list, n: int) -> tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    :param _sentences: List of sentences to select from.\n",
    "    :param n: Number of sentences to select.\n",
    "    :return: A tuple containing three lists:\n",
    "        - Sentences before the selected range.\n",
    "        - The selected sentences.\n",
    "        - Sentences after the selected range.\n",
    "    \"\"\"\n",
    "\n",
    "    _last_available_start = len(_sentences) - n\n",
    "    _start = random.randint(0, _last_available_start)\n",
    "    _end = _start + n\n",
    "\n",
    "    _sentences_before = _sentences[:_start]\n",
    "    _selected_sentences = _sentences[_start:_end]\n",
    "    _sentences_after = _sentences[_end:]\n",
    "\n",
    "    return _sentences_before, _selected_sentences, _sentences_after\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "7cbed72160d1c40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Storage for the modified texts alongside a helper function for adding a row of data.",
   "id": "c86bf9980d787e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ai_modified_texts = pd.DataFrame(columns=[\n",
    "    # The text before and after modification\n",
    "    \"text\",\n",
    "    \"modified_text\",  # The entire text with the modified sentences\n",
    "    \"modified_text_subset\",  # The response from our AI for modification\n",
    "    # The number of sentences before and after modification\n",
    "    \"num_sentences\",\n",
    "    \"modified_num_sentences\"\n",
    "    # Sentences before and after modification\n",
    "    \"sentences\",\n",
    "    \"modified_sentences\",\n",
    "    # The spans of the sentences before and after modification\n",
    "    \"sentence_spans\",\n",
    "    \"modified_sentence_spans\",\n",
    "    # If the span in the corresponding index is AI modified\n",
    "    # With reference to \"modified_sentence_spans\" only\n",
    "    \"span_ai_modified\",\n",
    "    # Is false if an error occurred\n",
    "    \"success\"\n",
    "])\n",
    "\n",
    "\n",
    "def add_row_to_ai_modified_texts(_original_row: pd.Series,\n",
    "                                 _modified_text: str | None,\n",
    "                                 _modified_text_subset: str | None,\n",
    "                                 _modified_num_sentences: int | None,\n",
    "                                 _modified_sentences: list | None,\n",
    "                                 _modified_sentence_spans: list | None,\n",
    "                                 _span_ai_modified: list | None,\n",
    "                                 _success: bool):\n",
    "    global ai_modified_texts\n",
    "\n",
    "    _new_row = pd.DataFrame(data={\n",
    "        \"text\": _original_row[\"text\"],\n",
    "        \"modified_text\": _modified_text,\n",
    "        \"modified_text_subset\": _modified_text_subset,\n",
    "        \"num_sentences\": _original_row[\"num_sentences\"],\n",
    "        \"modified_num_sentences\": _modified_num_sentences,\n",
    "        \"sentences\": [_original_row[\"sentences\"]],\n",
    "        \"modified_sentences\": [_modified_sentences],\n",
    "        \"sentence_spans\": [_original_row[\"sentence_spans\"]],\n",
    "        \"modified_sentence_spans\": [_modified_sentence_spans],\n",
    "        \"span_ai_modified\": [_span_ai_modified],\n",
    "        \"success\": _success\n",
    "    })\n",
    "\n",
    "    ai_modified_texts = pd.concat([ai_modified_texts, _new_row])\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "37fca6d0958a3b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A collection of definitions and helper functions with relation to the OpenAI API.",
   "id": "c1c052b7fc0ba599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with pathlib.Path(\"api_key.txt\").open() as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "CLIENT = OpenAI(api_key=api_key)\n",
    "\n",
    "END_PROMPT_TAG = \"<<END PROMPT>>\"\n",
    "START_TEXT = \"<<START TEXT>>\"\n",
    "MISSING_SENTENCES_TAG = \"<<MISSING SENTENCES>>\"\n",
    "MODIFICATION_ROLE = f\"\"\"\n",
    "You will received a prompt that was used to write a piece of text, followed by a token {END_PROMPT_TAG}.\n",
    "Then, I will indicate the number of sentences that need to be generated.\n",
    "After that, you will see a token {START_TEXT} marking the start of the text wherein a chunk of sentences are missing. The missing sentences are represented by a single token {MISSING_SENTENCES_TAG}.\n",
    "\n",
    "Using the prompt and the text before and after the {MISSING_SENTENCES_TAG} token, create the specified number of consecutive sentences to fill this gap.\n",
    "Ensure that your new sentences flow seamlessly with the surrounding text, maintaining the style, tone, and context.\n",
    "\n",
    "Please provide only the new sentences, with no additional formatting or notes.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def create_modification_prompt(_prompt_for_text: str, _num_missing_sentences: int,\n",
    "                               _text_before_missing: str, _text_after_missing) -> str:\n",
    "    _prompt = _prompt_for_text\n",
    "    _prompt += f\"\\n{END_PROMPT_TAG}\\n\"\n",
    "\n",
    "    _prompt += f\"Create {_num_missing_sentences} new sentences.\"\n",
    "\n",
    "    _prompt += f\"\\n{START_TEXT}\\n\"\n",
    "    _prompt += _text_before_missing\n",
    "    _prompt += f\"\\n{MISSING_SENTENCES_TAG}\\n\"\n",
    "    _prompt += _text_after_missing\n",
    "\n",
    "    return _prompt\n",
    "\n",
    "\n",
    "def send_prompt_with_role(_prompt: str, _role: str):\n",
    "    \"\"\"\n",
    "    :param _prompt: The prompt message to be sent to the chat model.\n",
    "    :param _role: The text for the role to be sent to the chat model.\n",
    "    :return: The response from the chat model, what is returned from `CLIENT.chat.completions.create`.\n",
    "    \"\"\"\n",
    "\n",
    "    _response = CLIENT.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": _role},\n",
    "            {\"role\": \"user\", \"content\": _prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return _response"
   ],
   "id": "c2bb2edffd67b7b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we modify the AI sentences.\n",
    "\n",
    "Here we are using a simplified prompt.  \n",
    "We need to select sentences to remove, then ask ChatGPT to regenerate them.  \n",
    "Finally we reconstruct the text with these new sentences.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Grab an entry to work with.\n",
    "2. Calculate the number of sentences to modify.\n",
    "3. Extract the sentences to be modified.\n",
    "4. Form a prompt to ChatGPT and send it.\n",
    "5. Acquire the response.\n",
    "6. Reform the text with the modified sentences.\n",
    "7. Perform some double checks to provide assurance that the modifications were successful.\n",
    "8. Save the modified text and associated information."
   ],
   "id": "211b7c37ee3cd668"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reset_skip_logging()\n",
    "\n",
    "# A dictionary for number of sentences to percentage of text to be modified multiplied by 10\n",
    "# That is if processing_info[25] is 20, then 20% of the text is to be modified\n",
    "# This value is incremented by 10 after use and reset to 10 after using 50.\n",
    "processing_info = {\n",
    "    i: 10\n",
    "    for i in range(MIN_NUM_SENTENCES, MAX_NUM_SENTENCES)\n",
    "}\n",
    "\n",
    "# Seed the RNG for consistency\n",
    "# At least with selecting the sentences\n",
    "random.seed(42)\n",
    "\n",
    "progress_bar = tqdm(total=len(to_be_ai_modified_texts),\n",
    "                    desc=\"Modifying texts\",\n",
    "                    postfix={\"skips\": num_skips})\n",
    "\n",
    "for row_index, (_, row) in enumerate(to_be_ai_modified_texts.iterrows()):\n",
    "    # Calculate the number of sentences to modify\n",
    "    percentage_to_be_modified = processing_info[row[\"num_sentences\"]]\n",
    "    percentage_to_be_modified /= 100\n",
    "    num_sentences_to_modify = round(percentage_to_be_modified * row[\"num_sentences\"])\n",
    "\n",
    "    # Update the number of sentences to modify\n",
    "    processing_info[row[\"num_sentences\"]] += 10\n",
    "    if processing_info[row[\"num_sentences\"]] > 50:\n",
    "        processing_info[row[\"num_sentences\"]] = 10\n",
    "\n",
    "    # Collect the sentences to be modified\n",
    "    sentences_with_spans: list[tuple[str, tuple[int, int]]]\n",
    "    sentences_with_spans = list(zip(row[\"sentences\"], row[\"sentence_spans\"]))\n",
    "    _selection_results = select_sentences(sentences_with_spans, num_sentences_to_modify)\n",
    "    # Some type hints to make PyCharm like me\n",
    "    sentences_before: list[tuple[str, tuple[int, int]]]\n",
    "    selected_sentences: list[tuple[str, tuple[int, int]]]\n",
    "    sentences_after: list[tuple[str, tuple[int, int]]]\n",
    "    # Unpack our tuple\n",
    "    sentences_before, selected_sentences, sentences_after = _selection_results\n",
    "\n",
    "    # Grab the text before and after the removed sentences\n",
    "    if sentences_before:\n",
    "        selected_sentences_start = selected_sentences[0][1][0]\n",
    "    else:\n",
    "        selected_sentences_start = 0\n",
    "\n",
    "    if sentences_after:\n",
    "        selected_sentences_end = selected_sentences[-1][1][1]\n",
    "    else:\n",
    "        selected_sentences_end = sentences_with_spans[-1][1][1]\n",
    "\n",
    "    text_before = row[\"text\"][:selected_sentences_start]\n",
    "    text_after = row[\"text\"][selected_sentences_end:]\n",
    "\n",
    "    # Create the prompt for ChatGPT\n",
    "    modification_prompt = create_modification_prompt(\n",
    "        row[\"prompt\"],\n",
    "        num_sentences_to_modify,\n",
    "        text_before,\n",
    "        text_after\n",
    "    )\n",
    "\n",
    "    # To our AI Overlord, ChatGPT\n",
    "    try:\n",
    "        modification_response = send_prompt_with_role(_prompt=modification_prompt, _role=MODIFICATION_ROLE)\n",
    "\n",
    "        # Extract our data from the response\n",
    "        modified_text_subset = modification_response.choices[0].message.content\n",
    "\n",
    "    except openai.BadRequestError:\n",
    "        log_row_skip(\"openai.BadRequestError\")\n",
    "        add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                     _modified_text=None,\n",
    "                                     _modified_text_subset=None,\n",
    "                                     _modified_num_sentences=None,\n",
    "                                     _modified_sentences=None,\n",
    "                                     _modified_sentence_spans=None,\n",
    "                                     _span_ai_modified=None,\n",
    "                                     _success=False)\n",
    "        # Update the progress bar\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(skips=num_skips)\n",
    "        continue\n",
    "\n",
    "    # Verify that we have an appropriate number of sentences\n",
    "    sentence_spans_in_response = list(tokenizer.span_tokenize(modified_text_subset))\n",
    "    sentences_in_response = [\n",
    "        modified_text_subset[start:stop] for start, stop in sentence_spans_in_response\n",
    "    ]\n",
    "    num_sentences_in_response = len(sentences_in_response)\n",
    "\n",
    "    num_sentences_in_response_variance = int(num_sentences_to_modify // 4)  # Allow a variance of 25% when receiving sentences\n",
    "    min_num_sentences_in_response = num_sentences_to_modify - num_sentences_in_response_variance\n",
    "    max_num_sentences_in_response = num_sentences_to_modify + num_sentences_in_response_variance\n",
    "\n",
    "    if num_sentences_in_response < min_num_sentences_in_response or num_sentences_in_response > max_num_sentences_in_response:\n",
    "        log_row_skip(\"Too large of a mismatch in received number of sentences\")\n",
    "        add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                     _modified_text=None,\n",
    "                                     _modified_text_subset=modified_text_subset,\n",
    "                                     _modified_num_sentences=None,\n",
    "                                     _modified_sentences=None,\n",
    "                                     _modified_sentence_spans=None,\n",
    "                                     _span_ai_modified=None,\n",
    "                                     _success=False)\n",
    "        # Update Mr. Progress Bar\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(skips=num_skips)\n",
    "        continue\n",
    "\n",
    "    # Recreate the text with the modified sentences in place\n",
    "    modified_text = text_before + modified_text_subset + text_after\n",
    "\n",
    "    # Now we regenerate the sentence spans and individual sentences from those spans\n",
    "    modified_text_sentence_spans = list(tokenizer.span_tokenize(modified_text))\n",
    "    modified_text_sentences = [\n",
    "        modified_text[start:stop] for start, stop in modified_text_sentence_spans\n",
    "    ]\n",
    "    modified_text_num_sentences = len(modified_text_sentences)\n",
    "\n",
    "    # Double check we have the expected number of sentences\n",
    "    expected_number_of_sentences = len(sentences_before) + num_sentences_in_response + len(sentences_after)\n",
    "    if modified_text_num_sentences != expected_number_of_sentences:\n",
    "        log_row_skip(\"Expected number of sentences after text regeneration does not match expected value\")\n",
    "        add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                     _modified_text=modified_text,\n",
    "                                     _modified_text_subset=modified_text_subset,\n",
    "                                     _modified_num_sentences=modified_text_num_sentences,\n",
    "                                     _modified_sentences=modified_text_sentences,\n",
    "                                     _modified_sentence_spans=modified_text_sentence_spans,\n",
    "                                     _span_ai_modified=None,\n",
    "                                     _success=False)\n",
    "        # Update the progress bar\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(skips=num_skips)\n",
    "        continue\n",
    "\n",
    "    # We know the number of sentences before and after the modifications themselves\n",
    "    # We know how many sentences there were in the modifications\n",
    "    # So we can create a list of bools for if a sentence was modified from that\n",
    "    is_sentence_modified = [False for _ in range(len(sentences_before))] \\\n",
    "                           + [True for _ in range(num_sentences_in_response)] \\\n",
    "                           + [False for _ in range(len(sentences_after))]\n",
    "\n",
    "    # We have a valid new text, add this to the data frame\n",
    "    add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                 _modified_text=modified_text,\n",
    "                                 _modified_text_subset=modified_text_subset,\n",
    "                                 _modified_num_sentences=modified_text_num_sentences,\n",
    "                                 _modified_sentences=modified_text_sentences,\n",
    "                                 _modified_sentence_spans=modified_text_sentence_spans,\n",
    "                                 _span_ai_modified=is_sentence_modified,\n",
    "                                 _success=True)\n",
    "\n",
    "    # The name's Bar, Progress Bar\n",
    "    progress_bar.update()\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Finished with {num_skips / len(to_be_ai_modified_texts):.2%} failed\")\n",
    "\n",
    "print_failures()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "e4e59842842c385a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print the data",
   "id": "2e720ccb093282db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ai_modified_texts",
   "id": "651d40e1204ad16e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we save the data to files.  \n",
    "I will only be saving those that were successfully modified."
   ],
   "id": "baa4a851f6c879a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_dir = pathlib.Path(\"./data/prepared\")\n",
    "\n",
    "original_texts_dir = save_dir / \"unmodified_texts\"\n",
    "ai_modified_texts_dir = save_dir / \"ai_modified_texts\"\n",
    "\n",
    "# Clear the directory if it exists\n",
    "if original_texts_dir.exists():\n",
    "    shutil.rmtree(original_texts_dir)\n",
    "if ai_modified_texts_dir.exists():\n",
    "    shutil.rmtree(ai_modified_texts_dir)\n",
    "\n",
    "# Recreate them\n",
    "original_texts_dir.mkdir()\n",
    "ai_modified_texts_dir.mkdir()\n",
    "\n",
    "# Save the original texts\n",
    "for i, (_, row) in enumerate(unmodified_texts.iterrows()):\n",
    "    # Make the data a little more usable\n",
    "    data = {\n",
    "        \"text\": row[\"text\"],\n",
    "        \"sentences\": row[\"sentences\"],\n",
    "        \"ai_modified\": [0 for _ in range(len(row[\"sentences\"]))]\n",
    "    }\n",
    "\n",
    "    file = original_texts_dir / f\"Text {i}.json\"\n",
    "    with file.open(\"w\") as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "# Save the AI modified texts\n",
    "i = 0\n",
    "for _, row in ai_modified_texts.iterrows():\n",
    "    if not row[\"success\"]:\n",
    "        continue\n",
    "\n",
    "    data = {\n",
    "        \"text\": row[\"modified_text\"],\n",
    "        \"sentences\": row[\"modified_sentences\"],\n",
    "        \"ai_modified\": row[\"span_ai_modified\"]\n",
    "    }\n",
    "\n",
    "    file = ai_modified_texts_dir / f\"Text {i}.json\"\n",
    "    i += 1\n",
    "    with file.open(\"w\") as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "5b146f9557a627c4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
