{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluating Data\n",
    "Using the prepared data, we evaluate the data."
   ],
   "id": "c7bdcc4968e841e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import socket\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scripts.ClientProtocol import ClientProtocol\n",
    "from scripts.Localizer import Localizer\n",
    "from scripts.MainProtocol import MainProtocol\n",
    "from scripts.fast_detect_gpt_start import fast_detect_gpt_start\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "e83fed8b74ecca20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants.",
   "id": "a1bf5f45412928a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The number of sentences when using the sliding window\n",
    "N_SENTENCES = 3\n",
    "\n",
    "# The location of our data\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "\n",
    "# The IP and Port to set up the TCP connection\n",
    "IP = \"localhost\"\n",
    "PORT = 8080\n",
    "\n",
    "# The seed to use for RNG\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# If we should use the better detection model\n",
    "# This is not noted anywhere in the data, so please take not manually\n",
    "# Also it makes the detector take much longer to start\n",
    "USE_gpt_j_6B = False\n",
    "# USE_gpt_j_6B = True"
   ],
   "id": "9547352c2226cea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we verify if we have prepared data.",
   "id": "1382e00f1a246e7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prepared_data_dir = DATA_DIR / \"prepared\"\n",
    "\n",
    "prepared_ai_modified_texts_dir = prepared_data_dir / \"ai_modified_texts\"\n",
    "prepared_unmodified_texts_dir = prepared_data_dir / \"unmodified_texts\"\n",
    "\n",
    "if not prepared_ai_modified_texts_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {prepared_ai_modified_texts_dir}\")\n",
    "if not prepared_unmodified_texts_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {prepared_unmodified_texts_dir}\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "7231b5c5662dc13a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Collecting the texts.",
   "id": "153a047b54813ea4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prepared_ai_modified_texts_files = []\n",
    "for file in prepared_ai_modified_texts_dir.iterdir():\n",
    "    prepared_ai_modified_texts_files.append(file)\n",
    "\n",
    "prepared_unmodified_texts_files = []\n",
    "for file in prepared_unmodified_texts_dir.iterdir():\n",
    "    prepared_unmodified_texts_files.append(file)\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "66e27519b03e7774",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Set up an evaluated directory.  \n",
    "Where we store the evaluated texts."
   ],
   "id": "79714a3ac4619c40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluated_dir = DATA_DIR / \"evaluated\"\n",
    "\n",
    "evaluated_ai_modified_texts_dir = evaluated_dir / \"ai_modified_texts\"\n",
    "evaluated_unmodified_texts_dir = evaluated_dir / \"unmodified_texts\"\n",
    "\n",
    "# Clear the directory if it exists\n",
    "if evaluated_ai_modified_texts_dir.exists():\n",
    "    shutil.rmtree(evaluated_ai_modified_texts_dir)\n",
    "if evaluated_unmodified_texts_dir.exists():\n",
    "    shutil.rmtree(evaluated_unmodified_texts_dir)\n",
    "\n",
    "# Recreate them\n",
    "evaluated_ai_modified_texts_dir.mkdir()\n",
    "evaluated_unmodified_texts_dir.mkdir()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "ca1f031af961729d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up the evaluator",
   "id": "4e44f26b2eb93a88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a socket for the evaluator to connect to\n",
    "my_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "my_sock.bind((IP, PORT))\n",
    "my_sock.listen()\n",
    "\n",
    "# Create the kwargs\n",
    "_evaluator_kwargs = {\n",
    "    \"--ip\": IP,\n",
    "    \"--port\": str(PORT)\n",
    "}\n",
    "if USE_gpt_j_6B:\n",
    "    _evaluator_kwargs[\"--reference_model_name\"] = \"gpt-j-6B\"\n",
    "\n",
    "# Start the evaluator\n",
    "evaluator_process = fast_detect_gpt_start(**_evaluator_kwargs)\n",
    "\n",
    "# Accept the evaluators connection\n",
    "evaluator_sock, _ = my_sock.accept()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "f16d6d39b217a36c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A bunch of helper functions for evaluating the texts.",
   "id": "5c5828434c7b2e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_text(_sock: socket.socket, _text: str) -> tuple[float, float]:\n",
    "    _sock.sendall(\n",
    "        MainProtocol.TEXT.create_message(_text)\n",
    "    )\n",
    "\n",
    "    _communication_id = ClientProtocol.read_indicator_from(_sock)\n",
    "    if _communication_id != ClientProtocol.EVALUATION:\n",
    "        raise ValueError(f\"Expected communication id {ClientProtocol.EVALUATION}, got {_communication_id}\")\n",
    "    _crit_and_prob_message = ClientProtocol.EVALUATION.read_rest_of_message_from(_sock)\n",
    "    _crit_and_prob = _crit_and_prob_message.data\n",
    "\n",
    "    return _crit_and_prob\n"
   ],
   "id": "960c62c16fabd83e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Evaluating the unmodified texts.\n",
    "\n",
    "First we load the data, then we localize it, then send it to the evaluator, and finally save it back to a file."
   ],
   "id": "723e5a7e771b7a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "skips_because_of_nan = 0\n",
    "\n",
    "# Okay, so starting with the unmodified texts\n",
    "for file in tqdm(prepared_unmodified_texts_files, desc=\"Evaluating unmodified texts\"):\n",
    "    # Grab the data from the file\n",
    "    with file.open() as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    span_to_evaluation = {}\n",
    "\n",
    "    # Create the localizer\n",
    "    localizer = Localizer(data[\"text\"])\n",
    "    # And now we evaluate\n",
    "    for localized_sentences, _, localized_sentences_sub_spans in localizer.iter_sentences(n_sentences=N_SENTENCES):\n",
    "        # Send the localized sentences to the evaluator and grab the result\n",
    "        crit_and_prob = evaluate_text(evaluator_sock, localized_sentences)\n",
    "\n",
    "        # Store it\n",
    "        for span in localized_sentences_sub_spans:\n",
    "            if span not in span_to_evaluation:\n",
    "                span_to_evaluation[span] = []\n",
    "            # Only care about the probability right now\n",
    "            span_to_evaluation[span].append(crit_and_prob[1])\n",
    "\n",
    "    # Convert the evaluations into something more usable\n",
    "    # The evaluations at index `i` correspond to the sentence at index `i`\n",
    "    sentence_evaluations = []\n",
    "    for span in sorted(span_to_evaluation.keys()):\n",
    "        sentence_evaluations.append(span_to_evaluation[span])\n",
    "\n",
    "    # Sometimes, for small inputs, the probability is nan\n",
    "    # In that case, we will skip this text\n",
    "    # The if statement in english \"if any of the values in sentence_evaluations is nan\"\n",
    "    if any([math.isnan(a) for a in itertools.chain.from_iterable(sentence_evaluations)]):\n",
    "        skips_because_of_nan += 1\n",
    "        continue\n",
    "\n",
    "    # We only want some of the data from the original data\n",
    "    evaluated_data = {\n",
    "        \"text\": data[\"text\"],\n",
    "        \"sentences\": data[\"sentences\"],\n",
    "        \"ai_modified\": data[\"ai_modified\"],\n",
    "        \"sentence_evaluations\": sentence_evaluations\n",
    "    }\n",
    "\n",
    "    # Finally we write a new file with the results\n",
    "    new_file = evaluated_unmodified_texts_dir / file.name\n",
    "    with new_file.open(\"w\") as f:\n",
    "        f.write(json.dumps(evaluated_data))\n",
    "\n",
    "print(f\"Number of texts skipped because of NaN: {skips_because_of_nan}\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "f843754a334ebc25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Repeat for the AI modified texts.",
   "id": "9284f1c6b7fbff57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "skips_because_of_nan = 0\n",
    "\n",
    "for file in tqdm(prepared_ai_modified_texts_files, desc=\"Evaluating AI modified texts\"):\n",
    "    with file.open() as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    span_to_evaluation = {}\n",
    "\n",
    "    localizer = Localizer(data[\"text\"])\n",
    "    for localized_sentences, _, localized_sentences_sub_spans in localizer.iter_sentences(n_sentences=N_SENTENCES):\n",
    "        crit_and_prob = evaluate_text(evaluator_sock, localized_sentences)\n",
    "\n",
    "        for span in localized_sentences_sub_spans:\n",
    "            if span not in span_to_evaluation:\n",
    "                span_to_evaluation[span] = []\n",
    "            span_to_evaluation[span].append(crit_and_prob[1])\n",
    "\n",
    "    sentence_evaluations = []\n",
    "    for span in sorted(span_to_evaluation.keys()):\n",
    "        sentence_evaluations.append(span_to_evaluation[span])\n",
    "\n",
    "    if any([math.isnan(a) for a in itertools.chain.from_iterable(sentence_evaluations)]):\n",
    "        skips_because_of_nan += 1\n",
    "        continue\n",
    "\n",
    "    evaluated_data = {\n",
    "        \"text\": data[\"text\"],\n",
    "        \"sentences\": data[\"sentences\"],\n",
    "        \"ai_modified\": data[\"ai_modified\"],\n",
    "        \"sentence_evaluations\": sentence_evaluations\n",
    "    }\n",
    "\n",
    "    new_file = evaluated_ai_modified_texts_dir / file.name\n",
    "    with new_file.open(\"w\") as f:\n",
    "        f.write(json.dumps(evaluated_data))\n",
    "\n",
    "print(f\"Number of texts skipped because of NaN: {skips_because_of_nan}\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "2d11f15dda49a8ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we terminate the evaluator as we no longer need it.",
   "id": "b35daed841718f73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator_sock.sendall(\n",
    "    MainProtocol.TERMINATE.create_message()\n",
    ")\n",
    "evaluator_sock.close()\n",
    "my_sock.close()\n",
    "evaluator_process.wait()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "71edfa0d7816fc97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we create a working set of the texts.\n",
    "\n",
    "This includes selecting 20% of the data for evaluation and the remaining 80% for fine-tuning.\n",
    "\n",
    "Also, there will most likely be a discrepancy in the number of unmodified texts and AI modified texts.  \n",
    "This will be due to the modification processing dropping any failed modifications.  \n",
    "So I will simply select the same number of unmodified texts as we have modified texts."
   ],
   "id": "bc6b5eb58bea4f6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "working_set_dir = DATA_DIR / \"working_set\"\n",
    "\n",
    "evaluation_set_dir = working_set_dir / \"evaluation_set\"\n",
    "fine_tuning_set_dir = working_set_dir / \"fine_tuning_set\"\n",
    "\n",
    "# Clear the directory if it exists\n",
    "if evaluation_set_dir.exists():\n",
    "    shutil.rmtree(evaluation_set_dir)\n",
    "if fine_tuning_set_dir.exists():\n",
    "    shutil.rmtree(fine_tuning_set_dir)\n",
    "\n",
    "# Recreate them\n",
    "evaluation_set_dir.mkdir()\n",
    "fine_tuning_set_dir.mkdir()\n",
    "\n",
    "# Grab our evaluated texts\n",
    "evaluated_ai_modified_texts_files = []\n",
    "for file in evaluated_ai_modified_texts_dir.iterdir():\n",
    "    evaluated_ai_modified_texts_files.append(file)\n",
    "evaluated_unmodified_texts_files = []\n",
    "for file in evaluated_unmodified_texts_dir.iterdir():\n",
    "    evaluated_unmodified_texts_files.append(file)\n",
    "\n",
    "# Trim down the unmodified texts to be the same length as the AI modified texts\n",
    "# Or just trim the longer one to the shorter one\n",
    "num_data = len(evaluated_ai_modified_texts_files)\n",
    "if len(evaluated_unmodified_texts_files) < num_data:\n",
    "    print(\"Number of unmodified texts is less than the AI modified texts\")\n",
    "\n",
    "evaluated_ai_modified_texts_files = random.sample(evaluated_ai_modified_texts_files, num_data)\n",
    "evaluated_unmodified_texts_files = random.sample(evaluated_unmodified_texts_files, num_data)\n",
    "\n",
    "# Now we select 20% to be used as evaluation data\n",
    "_20_percent = int(num_data * 0.2)\n",
    "evaluation_data_files = random.sample(evaluated_ai_modified_texts_files, _20_percent)\n",
    "evaluation_data_files += random.sample(evaluated_unmodified_texts_files, _20_percent)\n",
    "\n",
    "fine_tuning_data_files = [file for file in evaluated_ai_modified_texts_files + evaluated_unmodified_texts_files\n",
    "                          if file not in evaluation_data_files]\n",
    "\n",
    "# Finally we save these files to our new directories\n",
    "# With a little renaming so we as the users can still read things\n",
    "for file in evaluation_data_files:\n",
    "    _type = \"\"\n",
    "    if \"ai_modified_texts\" in file.parts:\n",
    "        _type = \"AI Modified\"\n",
    "    else:\n",
    "        _type = \"Unmodified\"\n",
    "\n",
    "    new_location = evaluation_set_dir / f\"{file.stem} - {_type}.json\"\n",
    "\n",
    "    shutil.copy(file, new_location)\n",
    "\n",
    "for file in fine_tuning_data_files:\n",
    "    _type = \"\"\n",
    "    if \"ai_modified_texts\" in file.parts:\n",
    "        _type = \"AI Modified\"\n",
    "    else:\n",
    "        _type = \"Unmodified\"\n",
    "\n",
    "    new_location = fine_tuning_set_dir / f\"{file.stem} - {_type}.json\"\n",
    "\n",
    "    shutil.copy(file, new_location)\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "58c72b6b957d9647",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
