{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing Data\n",
    "\n",
    "Here we grab the data used from the Human vs LLM Text Corpus, extract a subset we want to use, modify some of it and then save it to a series of files.  \n",
    "Not necessarily in that order, but you get the idea.\n"
   ],
   "id": "8d9ddb36e5263889"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "from typing import Literal\n",
    "\n",
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the csv file.",
   "id": "f071c62bf1c074a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(pathlib.Path(\"data/original/Human vs LLM Text Corpus/data.csv\"))\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "dbdd49e5deea0245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Extracting the data we want.\n",
    "\n",
    "* Remove all columns but the ones we care about.\n",
    "* Select only Human texts.\n",
    "* Cleaning the text a little."
   ],
   "id": "fb2c0cfa5cc1df8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Grab only the text and source columns\n",
    "human_sources = df.loc[:, [\"text\", \"source\"]]\n",
    "# Select only the human sources\n",
    "human_sources = human_sources[human_sources[\"source\"] == \"Human\"]\n",
    "\n",
    "# Strip the leading and trailing whitespace from the texts\n",
    "human_sources[\"text\"] = human_sources[\"text\"].str.strip()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "ef4ccccaaf718ee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adding a new column for each sentence and another for the number of sentences for each text.  \n",
    "This takes a bit because of the number of human texts."
   ],
   "id": "866c87c8f878d187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = nltk.tokenize.PunktTokenizer()\n",
    "\n",
    "\n",
    "def _modify(_row: pd.Series):\n",
    "    _sentence_spans = list(tokenizer.span_tokenize(_row[\"text\"]))\n",
    "    _sentences = [\n",
    "        _row[\"text\"][start:stop] for start, stop in _sentence_spans\n",
    "    ]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"sentence_spans\": _sentence_spans,\n",
    "        \"sentences\": _sentences\n",
    "    })\n",
    "\n",
    "\n",
    "# Create a row for the spans of the sentences and the actual sentences\n",
    "tqdm.pandas(desc=\"Tokenizing texts to sentences\")\n",
    "human_sources[[\"sentence_spans\", \"sentences\"]] = human_sources.progress_apply(\n",
    "    _modify,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# And another row for the number of sentences\n",
    "tqdm.pandas(desc=\"Counting the number of sentences in each text\")\n",
    "human_sources[\"num_sentences\"] = human_sources[\"sentences\"].apply(\n",
    "    lambda _sentences: len(_sentences)\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "5c8c7c59494ff52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Filter to the texts that we want.  \n",
    "That is texts with more than 40 sentences,\n",
    "50 of each with the number of sentences from 40 to 60,\n",
    "inclusive of 40, but not 60.  \n",
    "Why this?\n",
    "Well, because I wanted some even longish data."
   ],
   "id": "ff2e084358cff230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MIN_NUM_SENTENCES = 40\n",
    "MAX_NUM_SENTENCES = 60\n",
    "\n",
    "picked_sentences = pd.concat([\n",
    "    human_sources[human_sources[\"num_sentences\"] == num].sample(n=50, random_state=42)\n",
    "    for num in range(MIN_NUM_SENTENCES, MAX_NUM_SENTENCES)\n",
    "])\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "bb4db9cacbeae4c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we select 50% of the data to be modified in some way by an AI.",
   "id": "ecb5d38d9a7d14b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unmodified_texts = []\n",
    "to_be_ai_modified_texts = []\n",
    "\n",
    "for num in range(MIN_NUM_SENTENCES, MAX_NUM_SENTENCES):\n",
    "    subset = picked_sentences[picked_sentences[\"num_sentences\"] == num]\n",
    "\n",
    "    unmodified = subset.sample(frac=0.5, random_state=42)\n",
    "    to_be_ai_modified = subset.drop(unmodified.index)\n",
    "\n",
    "    unmodified_texts.append(unmodified)\n",
    "    to_be_ai_modified_texts.append(to_be_ai_modified)\n",
    "\n",
    "unmodified_texts = pd.concat(unmodified_texts)\n",
    "to_be_ai_modified_texts = pd.concat(to_be_ai_modified_texts)\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "1d2a212e6a2c478f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Due to working with a LLM, some of the response may not be in the correct format as requested.  \n",
    "This is a simple dat to track the types of failures that occur during modification."
   ],
   "id": "7af2532e44844e56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_skips = 0\n",
    "reasons_for_skipping_rows: dict[str, int] = {}\n",
    "last_skip_reason: str = \"\"\n",
    "\n",
    "\n",
    "def log_row_skip(_reason_for_skip: str):\n",
    "    global num_skips, last_skip_reason\n",
    "\n",
    "    if _reason_for_skip not in reasons_for_skipping_rows:\n",
    "        reasons_for_skipping_rows[_reason_for_skip] = 0\n",
    "    reasons_for_skipping_rows[_reason_for_skip] += 1\n",
    "\n",
    "    num_skips += 1\n",
    "    last_skip_reason = _reason_for_skip\n",
    "\n",
    "\n",
    "def print_failures():\n",
    "    print(\"Types of failures:\")\n",
    "    padding = max([\n",
    "        len(_key) for _key in reasons_for_skipping_rows.keys()\n",
    "    ])\n",
    "\n",
    "    for _key, _value in reasons_for_skipping_rows.items():\n",
    "        print(f\"\\t{_key:<{padding}} : {_value}\")\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "9f0e99b0798f599e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Helper function to select sentences to be modified.\n",
    "\n",
    "Either you can select sentences randomly, or in a cluster.\n",
    "This function simply selects `n` items from the given list."
   ],
   "id": "e52056b797b48e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def select_sentences(_sentences: list, n: int, select_by: Literal[\"group\", \"random\"]) -> list:\n",
    "    \"\"\"\n",
    "    :param _sentences: List of sentences to select from\n",
    "    :param n: Number of sentences to be selected\n",
    "    :param select_by: Criterion for selecting sentences, either 'group' or 'random'\n",
    "    :return: List of selected sentences\n",
    "    \"\"\"\n",
    "\n",
    "    if select_by == \"random\":\n",
    "        _sampled_sentences = random.sample(_sentences, n)\n",
    "        return _sampled_sentences\n",
    "\n",
    "    if select_by != \"group\":\n",
    "        raise ValueError(f\"Unknown select_by value: '{select_by}'\")\n",
    "\n",
    "    _last_available_start = len(_sentences) - n\n",
    "    _start = random.randint(0, _last_available_start)\n",
    "\n",
    "    _sampled_sentences = _sentences[_start:_start + n]\n",
    "    return _sampled_sentences\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "7cbed72160d1c40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Storage for the modified texts alongside a helper function for adding a row of data.",
   "id": "c86bf9980d787e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ai_modified_texts = pd.DataFrame(columns=[\n",
    "    # The text before and after modification\n",
    "    \"text\",\n",
    "    \"modified_text\",\n",
    "    # The number of sentences before and after modification\n",
    "    \"num_sentences\",\n",
    "    \"modified_num_sentences\"\n",
    "    # Sentences before and after modification\n",
    "    \"sentences\",\n",
    "    \"modified_sentences\",\n",
    "    # The spans of the sentences before and after modification\n",
    "    \"sentence_spans\",\n",
    "    \"modified_sentence_spans\",\n",
    "    # If the span in the corresponding index is AI modified\n",
    "    # With reference to \"modified_sentence_spans\" only\n",
    "    \"span_ai_modified\",\n",
    "    # Is false if an error occurred\n",
    "    \"success\"\n",
    "])\n",
    "\n",
    "\n",
    "def add_row_to_ai_modified_texts(_original_row: pd.Series,\n",
    "                                 _modified_text: str | None,\n",
    "                                 _modified_num_sentences: int | None,\n",
    "                                 _modified_sentences: list | None,\n",
    "                                 _modified_sentence_spans: list | None,\n",
    "                                 _span_ai_modified: list | None,\n",
    "                                 _success: bool):\n",
    "    global ai_modified_texts\n",
    "\n",
    "    _new_row = pd.DataFrame(data={\n",
    "        \"text\": _original_row[\"text\"],\n",
    "        \"modified_text\": _modified_text,\n",
    "        \"num_sentences\": _original_row[\"num_sentences\"],\n",
    "        \"modified_num_sentences\": _modified_num_sentences,\n",
    "        \"sentences\": [_original_row[\"sentences\"]],\n",
    "        \"modified_sentences\": [_modified_sentences],\n",
    "        \"sentence_spans\": [_original_row[\"sentence_spans\"]],\n",
    "        \"modified_sentence_spans\": [_modified_sentence_spans],\n",
    "        \"span_ai_modified\": [_span_ai_modified],\n",
    "        \"success\": _success\n",
    "    })\n",
    "\n",
    "    ai_modified_texts = pd.concat([ai_modified_texts, _new_row])\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "37fca6d0958a3b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A collection of definitions and helper functions with relation to the OpenAI API.",
   "id": "c1c052b7fc0ba599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with pathlib.Path(\"api_key.txt\").open() as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "CLIENT = OpenAI(api_key=api_key)\n",
    "\n",
    "END_TEXT_TAG = \"<<42 END TEXT 42>>\"\n",
    "END_SENTENCE_TAG = \"<<42 END SENTENCE 42>>\"\n",
    "MODIFICATION_ROLE = f\"\"\"\n",
    "I will give you text.\n",
    "Then a token {END_TEXT_TAG}.\n",
    "Followed by a series of sentences found in that text.\n",
    "Each sentence if followed by a token {END_SENTENCE_TAG}.\n",
    "\n",
    "You are to modify only those sentences, keeping the core meaning and intent the same.\n",
    "However, make more significant changes beyond simple word substitutions:\n",
    "    Rephrase sentences in a way to alter their structure or expression while maintaining the meaning.\n",
    "    You may change the sentence's style, using different phrasing, or by reordering information.\n",
    "    The formatting/whitespace for each sentence should be kept similar. For example if the sentence consists of a title section then a content section with two newlines inbetween, your response should contain the same.\n",
    "You are only to create one sentence for each sentence requested.\n",
    "\n",
    "Please provide the modified sentences in a JSON format:\n",
    "    [\"sentence1\", \"sentence2\", ...]\n",
    "in the same order they were received.\n",
    "Make sure to provide a valid JSON response.\n",
    "Return **only** this array with no additional formatting (e.g., no extra arrays, headers, or comments).\n",
    "\"\"\".strip()\n",
    "\n",
    "JSON_FIXER_ROLE = \"\"\"\n",
    "I will give you text that is supposed to be a valid JSON array of strings.\n",
    "However, the text may contain formatting errors such as:\n",
    "- Multiple arrays instead of one\n",
    "- Extra commas or brackets\n",
    "- Missing or extra quotation marks\n",
    "\n",
    "Your job is to fix these issues and return the text as a valid JSON array.\n",
    "Please ensure the response is strictly in the format:\n",
    "    [\"sentence1\", \"sentence2\", ...]\n",
    "and contains no errors.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def create_modification_prompt(_text: str, _sentences_to_modify: list[str]) -> str:\n",
    "    _prompt = _text\n",
    "    _prompt += f\"\\n{END_TEXT_TAG}\\n\"\n",
    "    for _sentence in _sentences_to_modify[:-1]:\n",
    "        _prompt += _sentence\n",
    "        _prompt += f\"\\n{END_SENTENCE_TAG}\\n\"\n",
    "    _prompt += _sentences_to_modify[-1]\n",
    "    _prompt += f\"\\n{END_SENTENCE_TAG}\"\n",
    "\n",
    "    return _prompt\n",
    "\n",
    "\n",
    "def send_prompt_with_role(_prompt: str, _role: str):\n",
    "    \"\"\"\n",
    "    :param _prompt: The prompt message to be sent to the chat model.\n",
    "    :param _role: The text for the role to be sent to the chat model.\n",
    "    :return: The response from the chat model, what is returned from `CLIENT.chat.completions.create`.\n",
    "    \"\"\"\n",
    "\n",
    "    _response = CLIENT.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": _role},\n",
    "            {\"role\": \"user\", \"content\": _prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return _response"
   ],
   "id": "c2bb2edffd67b7b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we modify the AI sentences.\n",
    "\n",
    "I have flattened this loop compared to the first version making this a bit easier to read and understand.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Grab an entry to work with.\n",
    "2. Calculate the number of sentences to modify.\n",
    "3. Extract the sentences to be modified.\n",
    "4. Form a prompt to ChatGPT and send it.\n",
    "5. Acquire the response.  \n",
    "   Attempt to decode it.  \n",
    "   Check we received the correct number of sentences.  \n",
    "   If this fails then skip.\n",
    "6. Reform the text with the modified sentences.\n",
    "7. Perform some double checks to provide assurance that the modifications were successful.\n",
    "8. Save the modified text and associated information."
   ],
   "id": "211b7c37ee3cd668"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A dictionary for number of sentences to percentage of text to be modified multiplied by 10\n",
    "# That is if processing_info[25] is 20, then 20% of the text is to be modified\n",
    "# This value is incremented by 10 after use and reset to 10 after using 50.\n",
    "processing_info = {\n",
    "    i: 10\n",
    "    for i in range(MIN_NUM_SENTENCES, MAX_NUM_SENTENCES)\n",
    "}\n",
    "\n",
    "# Seed the RNG for consistency\n",
    "# At least with selecting the sentences\n",
    "random.seed(42)\n",
    "\n",
    "progress_bar = tqdm(total=len(to_be_ai_modified_texts),\n",
    "                    desc=\"Modifying texts\",\n",
    "                    postfix={\"skips\": num_skips})\n",
    "\n",
    "for _, row in to_be_ai_modified_texts.iterrows():\n",
    "    # Calculate the number of sentences to modify\n",
    "    percentage_to_be_modified = processing_info[row[\"num_sentences\"]]\n",
    "    percentage_to_be_modified /= 100\n",
    "    num_sentences_to_modify = round(percentage_to_be_modified * row[\"num_sentences\"])\n",
    "\n",
    "    # Update the number of sentences to modify\n",
    "    processing_info[row[\"num_sentences\"]] += 10\n",
    "    if processing_info[row[\"num_sentences\"]] > 50:\n",
    "        processing_info[row[\"num_sentences\"]] = 10\n",
    "\n",
    "    # Collect the sentences to be modified\n",
    "    sentences_with_spans = list(zip(row[\"sentences\"], row[\"sentence_spans\"]))\n",
    "    sampled_sentences = select_sentences(sentences_with_spans, num_sentences_to_modify, select_by=\"group\")\n",
    "    # sampled_sentences = select_sentences(sentences_with_spans, num_sentences_to_modify, select_by=\"random\")\n",
    "    sentences_to_modify, sentence_spans_to_modify = zip(*sampled_sentences)\n",
    "\n",
    "    # Create the prompt for ChatGPT\n",
    "    modification_prompt = create_modification_prompt(row[\"text\"], sentences_to_modify)\n",
    "\n",
    "    # To our AI Overlord, ChatGPT\n",
    "    try:\n",
    "        modification_response = send_prompt_with_role(_prompt=modification_prompt, _role=MODIFICATION_ROLE)\n",
    "\n",
    "        # Extract our data from the response\n",
    "        modification_response_json_text = modification_response.choices[0].message.content\n",
    "        response_list = json.loads(modification_response_json_text)\n",
    "\n",
    "    except openai.BadRequestError:\n",
    "        log_row_skip(\"openai.BadRequestError\")\n",
    "        response_list = []\n",
    "    except json.JSONDecodeError:\n",
    "        # Attempt to fix this decode error with another request to ChatGPT\n",
    "        try:\n",
    "            fix_response = send_prompt_with_role(\n",
    "                # Name 'modification_response_json_text' can be undefined\n",
    "                _prompt=modification_response_json_text,  # NOQA\n",
    "                _role=JSON_FIXER_ROLE)\n",
    "\n",
    "            # Extract the new data from the response\n",
    "            fix_response_json_text = fix_response.choices[0].message.content\n",
    "            response_list = json.loads(fix_response_json_text)\n",
    "\n",
    "        except openai.BadRequestError:\n",
    "            log_row_skip(\"openai.BadRequestError\")\n",
    "            response_list = []\n",
    "        except json.JSONDecodeError:\n",
    "            log_row_skip(\"json.JSONDecodeError\")\n",
    "            response_list = []\n",
    "\n",
    "    # Making sure that the correct number of sentences was received\n",
    "    if response_list and (len(response_list) != num_sentences_to_modify):\n",
    "        log_row_skip(\"Wrong number of sentences received\")\n",
    "        response_list = []\n",
    "\n",
    "    # Stop here if we have a failed request\n",
    "    if not response_list:\n",
    "        add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                     _modified_text=None,\n",
    "                                     _modified_num_sentences=None,\n",
    "                                     _modified_sentences=None,\n",
    "                                     _modified_sentence_spans=None,\n",
    "                                     _span_ai_modified=None,\n",
    "                                     _success=False)\n",
    "        # Update the progress abr\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(skips=num_skips)\n",
    "        continue\n",
    "\n",
    "    # Collect the whitespace inbetween the normal sentences\n",
    "    whitespace_between_sentences = [\n",
    "        row[\"text\"][start:stop]\n",
    "        for start, stop in zip(\n",
    "            [a for _, a in row[\"sentence_spans\"][:-1]],\n",
    "            [b for b, _ in row[\"sentence_spans\"][1:]]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Create a list of new sentences to form our text\n",
    "    new_sentences = []\n",
    "    is_sentence_modified = []\n",
    "    for i, sentence_span in enumerate(row[\"sentence_spans\"]):\n",
    "        if sentence_span in sentence_spans_to_modify:\n",
    "            is_sentence_modified.append(True)\n",
    "            new_sentences.append(\n",
    "                response_list[sentence_spans_to_modify.index(sentence_span)]\n",
    "            )\n",
    "        else:\n",
    "            is_sentence_modified.append(False)\n",
    "            new_sentences.append(row[\"sentences\"][i])\n",
    "\n",
    "    # Regenerate the text using the modified sentences and the collected whitespace\n",
    "    new_sentences_and_whitespace = []\n",
    "    for sentence, whitespace in zip(new_sentences, whitespace_between_sentences):\n",
    "        new_sentences_and_whitespace.append(sentence)\n",
    "        new_sentences_and_whitespace.append(whitespace)\n",
    "    new_sentences_and_whitespace.append(new_sentences[-1])\n",
    "    modified_text = \"\".join(new_sentences_and_whitespace)\n",
    "\n",
    "    # Now we regenerate the sentence spans and individual sentences from those spans\n",
    "    modified_text_sentence_spans = list(tokenizer.span_tokenize(modified_text))\n",
    "    modified_text_sentences = [\n",
    "        modified_text[start:stop] for start, stop in modified_text_sentence_spans\n",
    "    ]\n",
    "    modified_text_num_sentences = len(modified_text_sentences)\n",
    "\n",
    "    # Double-check our sentences after tokenization match the ones before\n",
    "    # Sorry about the walrus\n",
    "    if failure_matching_sentences_after_re_tokenization := (new_sentences != modified_text_sentences):\n",
    "        log_row_skip(\"New sentences and modified text sentences don't match after re-tokenization\")\n",
    "\n",
    "    # Double check we have the correct number of sentences\n",
    "    if failure_number_sentences_after_modification := (row[\"num_sentences\"] != modified_text_num_sentences):\n",
    "        log_row_skip(\"Number of sentences after modification don't match before modification\")\n",
    "\n",
    "    if failure_matching_sentences_after_re_tokenization or failure_number_sentences_after_modification:\n",
    "        add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                     _modified_text=modified_text,\n",
    "                                     _modified_num_sentences=modified_text_num_sentences,\n",
    "                                     _modified_sentences=modified_text_sentences,\n",
    "                                     _modified_sentence_spans=modified_text_sentence_spans,\n",
    "                                     _span_ai_modified=None,\n",
    "                                     _success=False)\n",
    "        # Update Mr. Progress Bar\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(skips=num_skips)\n",
    "        continue\n",
    "\n",
    "    # We have a valid new text, add this to the data frame\n",
    "    add_row_to_ai_modified_texts(_original_row=row,\n",
    "                                 _modified_text=modified_text,\n",
    "                                 _modified_num_sentences=modified_text_num_sentences,\n",
    "                                 _modified_sentences=modified_text_sentences,\n",
    "                                 _modified_sentence_spans=modified_text_sentence_spans,\n",
    "                                 _span_ai_modified=is_sentence_modified,\n",
    "                                 _success=True)\n",
    "\n",
    "    progress_bar.update()\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Finished with {num_skips / len(to_be_ai_modified_texts):.2%} failed\")\n",
    "\n",
    "print_failures()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "23ca878179b2ff08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print the data",
   "id": "2e720ccb093282db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ai_modified_texts",
   "id": "651d40e1204ad16e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we save the data to files.  \n",
    "I will only be saving those that were successfully modified."
   ],
   "id": "baa4a851f6c879a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_dir = pathlib.Path(\"./data/prepared\")\n",
    "\n",
    "original_texts_dir = save_dir / \"unmodified_texts\"\n",
    "ai_modified_texts_dir = save_dir / \"ai_modified_texts\"\n",
    "\n",
    "# Clear the directory if it exists\n",
    "if original_texts_dir.exists():\n",
    "    shutil.rmtree(original_texts_dir)\n",
    "if ai_modified_texts_dir.exists():\n",
    "    shutil.rmtree(ai_modified_texts_dir)\n",
    "\n",
    "# Recreate them\n",
    "original_texts_dir.mkdir()\n",
    "ai_modified_texts_dir.mkdir()\n",
    "\n",
    "# Save the original texts\n",
    "for i, (_, row) in enumerate(unmodified_texts.iterrows()):\n",
    "    # Make the data a little more usable\n",
    "    data = {\n",
    "        \"text\": row[\"text\"],\n",
    "        \"sentences\": row[\"sentences\"],\n",
    "        \"ai_modified\": [0 for _ in range(len(row[\"sentences\"]))]\n",
    "    }\n",
    "\n",
    "    file = original_texts_dir / f\"Text {i}.json\"\n",
    "    with file.open(\"w\") as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "# Save the AI modified texts\n",
    "i = 0\n",
    "for _, row in ai_modified_texts.iterrows():\n",
    "    if not row[\"success\"]:\n",
    "        continue\n",
    "\n",
    "    data = {\n",
    "        \"text\": row[\"modified_text\"],\n",
    "        \"sentences\": row[\"modified_sentences\"],\n",
    "        \"ai_modified\": row[\"span_ai_modified\"]\n",
    "    }\n",
    "\n",
    "    file = ai_modified_texts_dir / f\"Text {i}.json\"\n",
    "    i += 1\n",
    "    with file.open(\"w\") as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "5b146f9557a627c4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
