{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing Data\n",
    "\n",
    "Here we grab the data used from the Human vs LLM Text Corpus, extract a subset we want to use, modify some of it and then save it to a series of files.  \n"
   ],
   "id": "8d9ddb36e5263889"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the csv file.",
   "id": "f071c62bf1c074a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(pathlib.Path(\"data/original/Human vs LLM Text Corpus/data.csv\"))\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "dbdd49e5deea0245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Extracting the data we want.\n",
    "\n",
    "* Remove all columns but the ones we care about.\n",
    "* Select only Human texts\n",
    "* Remove the source"
   ],
   "id": "fb2c0cfa5cc1df8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Grab only the text and source columns\n",
    "human_sources = df.loc[:, [\"text\", \"source\"]]\n",
    "# Select only the human sources\n",
    "human_sources = human_sources[human_sources[\"source\"] == \"Human\"]\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "ef4ccccaaf718ee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adding a new column for each sentence and another for the number of sentences for each text.  \n",
    "This takes a bit because of the number of human texts."
   ],
   "id": "866c87c8f878d187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = nltk.tokenize.PunktTokenizer()\n",
    "\n",
    "\n",
    "def _modify(_row: pd.Series):\n",
    "    _sentence_spans = list(tokenizer.span_tokenize(_row[\"text\"]))\n",
    "    _sentences = [\n",
    "        _row[\"text\"][start:stop] for start, stop in _sentence_spans\n",
    "    ]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"sentence_spans\": _sentence_spans,\n",
    "        \"sentences\": _sentences\n",
    "    })\n",
    "\n",
    "\n",
    "# Create a row for the spans of the sentences and the actual sentences\n",
    "tqdm.pandas(desc=\"Tokenizing texts to sentences\")\n",
    "human_sources[[\"sentence_spans\", \"sentences\"]] = human_sources.progress_apply(\n",
    "    _modify,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# And another row for the number of sentences\n",
    "tqdm.pandas(desc=\"Counting the number of sentences in each text\")\n",
    "human_sources[\"num_sentences\"] = human_sources[\"sentences\"].apply(\n",
    "    lambda _sentences: len(_sentences)\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "5c8c7c59494ff52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Filter to the texts that we want.  \n",
    "That is texts with more than 40 sentences,\n",
    "50 of each with the number of sentences from 40 to 60,\n",
    "inclusive of 40, but not 60.  \n",
    "Why this?\n",
    "Well, because I wanted some even longish data."
   ],
   "id": "ff2e084358cff230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "picked_sentences = pd.concat([\n",
    "    human_sources[human_sources[\"num_sentences\"] == num].sample(n=50, random_state=42)\n",
    "    for num in range(40, 60)\n",
    "])\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "bb4db9cacbeae4c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we select 50% of the data to be modified in some way by an AI.",
   "id": "ecb5d38d9a7d14b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unmodified_texts = []\n",
    "to_be_ai_modified_texts = []\n",
    "\n",
    "for num in range(40, 60):\n",
    "    subset = picked_sentences[picked_sentences[\"num_sentences\"] == num]\n",
    "\n",
    "    unmodified = subset.sample(frac=0.5, random_state=42)\n",
    "    to_be_ai_modified = subset.drop(unmodified.index)\n",
    "\n",
    "    unmodified_texts.append(unmodified)\n",
    "    to_be_ai_modified_texts.append(to_be_ai_modified)\n",
    "\n",
    "unmodified_texts = pd.concat(unmodified_texts)\n",
    "to_be_ai_modified_texts = pd.concat(to_be_ai_modified_texts)\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "1d2a212e6a2c478f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we modify the AI sentences.\n",
    "\n",
    "So steps in English as this is a doozy:\n",
    "\n",
    "1. Grab all sentences of a specific length\n",
    "2. Separate those sentences into 5 subsets  \n",
    "   Each subset has a different amount modified\n",
    "3. Find the number of sentences to modify, lets say n\n",
    "4. Randomly select n number of sentences to modify\n",
    "5. Form a prompt to ChatGPT asking it to modify the n chosen sentences\n",
    "6. Extract the modified sentences from ChatGPT  \n",
    "   Skip if there is an error or mismatch in the received number of sentences\n",
    "7. Recreate the text with the AI modified sentences preserving the whitespace\n",
    "8. Add an entry for this text into a dataframe for later use"
   ],
   "id": "211b7c37ee3cd668"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with pathlib.Path(\"api_key.txt\").open() as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# MODEL = \"gpt-4o-mini\"\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "CLIENT = OpenAI(api_key=api_key)\n",
    "ROLE = \"\"\"I will give you text.\n",
    "Then a token <<42 END TEXT 42>>.\n",
    "Followed by a series of sentences found in that text.\n",
    "Each sentence if followed by a token <<42 END SENTENCE 42>>.\n",
    "The formatting/whitespace for each sentence should be kept similar.\n",
    "\n",
    "You are to modify only those sentences, keeping the content the same.\n",
    "You are only to create one sentence for each sentence requested.\n",
    "\n",
    "Please provide the modified sentences in a json format:\n",
    "    [\"sentence1\", \"sentence2\", ...]\n",
    "in the same order they were received.\n",
    "With no other formatting such as ```json ... ```\n",
    "\"\"\"\n",
    "\n",
    "ai_modified_texts = pd.DataFrame(columns=[\"text\",\n",
    "                                          \"num_sentences\",\n",
    "                                          \"sentences\",\n",
    "                                          \"sentence_spans\",\n",
    "                                          \"span_ai_modified\",  # If the span in the corresponding index is AI modified\n",
    "                                          \"success\"  # Is false if an error occurred\n",
    "                                          ])\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "fails = 0\n",
    "successes = 0\n",
    "iterations = 0\n",
    "\n",
    "progress_bar = tqdm(total=len(to_be_ai_modified_texts), desc=\"Modifying texts\",\n",
    "                    postfix={\n",
    "                        \"fails\": fails,\n",
    "                        \"successes\": successes\n",
    "                    })\n",
    "\n",
    "last_failure = \"\"\n",
    "types_of_failures = {}\n",
    "\n",
    "\n",
    "def add_failure(failure: str):\n",
    "    global last_failure\n",
    "\n",
    "    if failure not in types_of_failures:\n",
    "        types_of_failures[failure] = 0\n",
    "    types_of_failures[failure] += 1\n",
    "\n",
    "    last_failure = failure\n",
    "\n",
    "\n",
    "for num_sentences in range(40, 60):\n",
    "    # Grab the subset we want\n",
    "    subset = to_be_ai_modified_texts[to_be_ai_modified_texts[\"num_sentences\"] == num_sentences]\n",
    "\n",
    "    # Split this into 5 equal distinct sets to be modified\n",
    "    # Index 0 is to have 10% modified, index 1 is 20% and so on\n",
    "    sample_sets = []\n",
    "    num_per_sample = len(subset) // 5\n",
    "    for _ in range(5):\n",
    "        sample_sets.append(\n",
    "            subset.sample(n=num_per_sample, random_state=42)\n",
    "        )\n",
    "        subset = subset.drop(sample_sets[-1].index)\n",
    "\n",
    "    # Now we need to modify the texts\n",
    "    # The first entry into sub_subsets is the one to be 10% modified, the second 20% and so on\n",
    "    for index, percentage_modification in enumerate(range(10, 60, 10)):\n",
    "        working_set = sample_sets[index]\n",
    "\n",
    "        # Turn this into an actual percentage\n",
    "        percentage_modification /= 100\n",
    "        # And into the number of sentences to sample\n",
    "        num_sentences_to_modify = round(percentage_modification * num_sentences)\n",
    "\n",
    "        for _, row in working_set.iterrows():\n",
    "            iterations += 1\n",
    "\n",
    "            # Select a percentage of the sentences to modify\n",
    "            sentences_with_spans = list(zip(row[\"sentences\"], row[\"sentence_spans\"]))\n",
    "            sampled_sentences_with_spans = random.sample(sentences_with_spans, k=num_sentences_to_modify)\n",
    "            # Sprinkle in some python black magic\n",
    "            sentences_to_modify, sentence_spans_to_modify = zip(*sampled_sentences_with_spans)\n",
    "\n",
    "            # Create the prompt for ChatGPT\n",
    "            prompt = row[\"text\"]\n",
    "            prompt += \"\\n<<42 END TEXT 42>>\\n\"\n",
    "            for sentence in sentences_to_modify[:-1]:\n",
    "                prompt += sentence\n",
    "                prompt += \"\\n<<42 END SENTENCE 42>>\\n\"\n",
    "            prompt += sentences_to_modify[-1]\n",
    "            prompt += \"\\n<<42 END SENTENCE 42>>\"\n",
    "\n",
    "            # Send to ChatGPT\n",
    "            try:\n",
    "                response = CLIENT.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": ROLE},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Grab the content of the response\n",
    "                response_json_text = response.choices[0].message.content\n",
    "                response_list = json.loads(response_json_text)\n",
    "            except openai.BadRequestError:\n",
    "                add_failure(\"openai.BadRequestError\")\n",
    "                response_list = []\n",
    "            except json.JSONDecodeError:\n",
    "                add_failure(\"json.JSONDecodeError\")\n",
    "                response_list = []\n",
    "\n",
    "            # Make sure we received the correct number of sentences\n",
    "            if response_list and len(response_list) != len(sentences_to_modify):\n",
    "                add_failure(\"Mismatch in number of sentences in response\")\n",
    "                response_list = []\n",
    "\n",
    "            # Handling a failed request\n",
    "            if not response_list:\n",
    "                # Leave in the old data\n",
    "                failed_row = pd.DataFrame(data={\n",
    "                    \"text\": row[\"text\"],\n",
    "                    \"num_sentences\": row[\"num_sentences\"],\n",
    "                    \"sentences\": [row[\"sentences\"]],\n",
    "                    \"sentence_spans\": [row[\"sentence_spans\"]],\n",
    "                    \"span_ai_modified\": [[False for _ in range(row[\"num_sentences\"])]],\n",
    "                    \"success\": False\n",
    "                })\n",
    "                ai_modified_texts = pd.concat([ai_modified_texts, failed_row])\n",
    "                # Update the progress bar\n",
    "                fails += 1\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_postfix(fails=fails, successes=successes)\n",
    "                continue\n",
    "\n",
    "            # Now we modify the original text to include the AI modified sentences\n",
    "            # I wish to preserve the whitespace between the sentences\n",
    "            # Hence why the following is unnecessarily complicated\n",
    "\n",
    "            # Extract the space between spans\n",
    "            the_stuff_in_between_spans = [\n",
    "                row[\"text\"][start:stop]\n",
    "                for start, stop in zip(\n",
    "                    [a for _, a in row[\"sentence_spans\"][:-1]],\n",
    "                    [b for b, _ in row[\"sentence_spans\"][1:]]\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Now create a new list of sentences, replacing the modified sentences\n",
    "            new_sentences = []\n",
    "            is_sentence_modified = []\n",
    "            for i, sentence_span in enumerate(row[\"sentence_spans\"]):\n",
    "                if sentence_span in sentence_spans_to_modify:\n",
    "                    is_sentence_modified.append(True)\n",
    "                    new_sentences.append(\n",
    "                        response_list[sentence_spans_to_modify.index(sentence_span)]\n",
    "                    )\n",
    "                else:\n",
    "                    is_sentence_modified.append(False)\n",
    "                    new_sentences.append(\n",
    "                        row[\"sentences\"][i]\n",
    "                    )\n",
    "\n",
    "            # Finally we zip the new sentences together with the old whitespace\n",
    "            result = []\n",
    "            for item1, item2, in zip(new_sentences, the_stuff_in_between_spans):\n",
    "                result.append(item1)\n",
    "                result.append(item2)\n",
    "            result.append(new_sentences[-1])\n",
    "            modified_text = \"\".join(result)\n",
    "\n",
    "            # After all that, we have received our modified text\n",
    "            # Now regenerate the extra data\n",
    "            modified_text_sentence_spans = list(tokenizer.span_tokenize(modified_text))\n",
    "            modified_text_sentences = [\n",
    "                modified_text[start:stop] for start, stop in modified_text_sentence_spans\n",
    "            ]\n",
    "            modified_text_num_sentences = len(modified_text_sentences)\n",
    "\n",
    "            if new_sentences != modified_text_sentences:\n",
    "                add_failure(\"Mismatch from combining then tokenizing\")\n",
    "                failed_row = pd.DataFrame(data={\n",
    "                    \"text\": row[\"text\"],\n",
    "                    \"num_sentences\": modified_text_num_sentences,\n",
    "                    \"sentences\": [modified_text_sentences],\n",
    "                    \"sentence_spans\": [modified_text_sentence_spans],\n",
    "                    \"span_ai_modified\": [None],\n",
    "                    \"success\": False\n",
    "                })\n",
    "                # Update the progress bar\n",
    "                fails += 1\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_postfix(fails=fails, successes=successes)\n",
    "                continue\n",
    "\n",
    "            if row[\"num_sentences\"] != modified_text_num_sentences:\n",
    "                add_failure(\"Mismatch in number of sentences in modified text\")\n",
    "                failed_row = pd.DataFrame(data={\n",
    "                    \"text\": row[\"text\"],\n",
    "                    \"num_sentences\": modified_text_num_sentences,\n",
    "                    \"sentences\": [modified_text_sentences],\n",
    "                    \"sentence_spans\": [modified_text_sentence_spans],\n",
    "                    \"span_ai_modified\": [None],\n",
    "                    \"success\": False\n",
    "                })\n",
    "                ai_modified_texts = pd.concat([ai_modified_texts, failed_row])\n",
    "                # Update the progress bar\n",
    "                fails += 1\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_postfix(fails=fails, successes=successes)\n",
    "                continue\n",
    "\n",
    "            # And add to the dataframe\n",
    "            # print(\"Success\")\n",
    "            new_row = pd.DataFrame(data={\n",
    "                \"text\": modified_text,\n",
    "                \"num_sentences\": modified_text_num_sentences,\n",
    "                \"sentences\": [modified_text_sentences],\n",
    "                \"sentence_spans\": [modified_text_sentence_spans],\n",
    "                \"span_ai_modified\": [is_sentence_modified],\n",
    "                \"success\": True\n",
    "            })\n",
    "            ai_modified_texts = pd.concat([ai_modified_texts, new_row])\n",
    "            # Update the progress bar\n",
    "            successes += 1\n",
    "            progress_bar.update()\n",
    "            progress_bar.set_postfix(fails=fails, successes=successes)\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Finished with {fails / len(to_be_ai_modified_texts):.2%} failed\")\n",
    "print(f\"Iterations: {iterations}\")\n",
    "\n",
    "print(\"Types of failures:\")\n",
    "for key, value in types_of_failures.items():\n",
    "    print(f\"\\t{key}: {value}\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "23ca878179b2ff08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print the data",
   "id": "2e720ccb093282db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ai_modified_texts",
   "id": "651d40e1204ad16e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we save the data to files.  \n",
    "I will only be saving those that were successfully modified."
   ],
   "id": "baa4a851f6c879a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_dir = pathlib.Path(\"./data/prepared\")\n",
    "\n",
    "original_texts_dir = save_dir / \"unmodified_texts\"\n",
    "ai_modified_texts_dir = save_dir / \"ai_modified_texts\"\n",
    "\n",
    "# Clear the directory if it exists\n",
    "if original_texts_dir.exists():\n",
    "    shutil.rmtree(original_texts_dir)\n",
    "if ai_modified_texts_dir.exists():\n",
    "    shutil.rmtree(ai_modified_texts_dir)\n",
    "\n",
    "# Recreate them\n",
    "original_texts_dir.mkdir()\n",
    "ai_modified_texts_dir.mkdir()\n",
    "\n",
    "# Save the original texts\n",
    "for i, (_, row) in enumerate(unmodified_texts.iterrows()):\n",
    "    # Make the data a little more usable\n",
    "    sentences = nltk.sent_tokenize(row[\"text\"])\n",
    "    data = {\n",
    "        \"text\": row[\"text\"],\n",
    "        \"sentences\": sentences,\n",
    "        \"ai_modified\": [0 for _ in range(len(sentences))]\n",
    "    }\n",
    "\n",
    "    file = original_texts_dir / f\"Text {i}.json\"\n",
    "    with file.open(\"w\") as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "# Save the AI modified texts\n",
    "i = 0\n",
    "for _, row in ai_modified_texts.iterrows():\n",
    "    if not row[\"success\"]:\n",
    "        continue\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(row[\"text\"])\n",
    "    data = {\n",
    "        \"text\": row[\"text\"],\n",
    "        \"sentences\": row[\"sentences\"],\n",
    "        \"ai_modified\": row[\"span_ai_modified\"]\n",
    "    }\n",
    "    \n",
    "    file = ai_modified_texts_dir / f\"Text {i}.json\"\n",
    "    i += 1\n",
    "    with file.open(\"w\") as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "print(\"Done\")"
   ],
   "id": "5b146f9557a627c4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
